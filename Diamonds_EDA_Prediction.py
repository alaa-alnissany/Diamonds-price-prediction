# -*- coding: utf-8 -*-
"""diamonds_alaa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R9LSGpOSU91ufmI93e9qKCsJPjH6ZJd2

# Importing libraries

Main libraries
"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns

"""Sklearn tools"""

# Categorical Features encoding
from sklearn.preprocessing import OrdinalEncoder , OneHotEncoder ,  LabelEncoder

# Feature scale
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import  minmax_scale
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer

# Pipline
from sklearn.base import BaseEstimator , TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.multioutput import RegressorChain

# Models
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Ensempling models
from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor, ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor,VotingRegressor,HistGradientBoostingRegressor


# Loss function 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Model tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

"""# Get Diamonds Dataset"""

from google.colab import drive
drive.mount('/content/drive')

diamonds_path = "./drive/MyDrive/diamonds/diamonds.csv"
diamonds = pd.DataFrame(pd.read_csv(diamonds_path))

"""# Data checking"""

diamonds.info()

diamonds.describe()

"""We note that there are at least one zero value in every single column of ["x","y","z"], so we need to replace sparce features with nan value to estimate them later."""

diamonds = diamonds.applymap(lambda x: np.NaN if x==0 else x)

diamonds.info()

"""Checking Categorical Features values and their frequency"""

print("<<<<<<<<<<<<<<<<<<<<<<cut values>>>>>>>>>>>>>>>>>>>>>>>>>")
print(diamonds["cut"].value_counts())
print("\n<<<<<<<<<<<<<<<<<<<<<<color values>>>>>>>>>>>>>>>>>>>>>>>>>")
print(diamonds["color"].value_counts())
print("\n<<<<<<<<<<<<<<<<<<<<<<clarity values>>>>>>>>>>>>>>>>>>>>>>>>>")
print(diamonds["clarity"].value_counts())

"""# Ploting Data:"""

max_per_cut = diamonds.groupby("cut").max()
mean_per_cut = diamonds.groupby("cut").mean()
median_per_cut = diamonds.groupby("cut").median()

fig,ax = plt.subplots(1,3,figsize=(15, 5))
sns.scatterplot(data = mean_per_cut, x = "cut", y = "price", ax = ax[0])
ax[0].set_title("mean")
sns.scatterplot(data = median_per_cut, x = "cut",y = "price", color = "r", ax = ax[1])
ax[1].set_title("median")
sns.scatterplot(data = max_per_cut, x = "cut", y = "price", color = "g", ax = ax[2])
ax[2].set_title("max")
plt.show()

median_per_cut

max_per_color = diamonds.groupby("color").max()
mean_per_color = diamonds.groupby("color").mean()
median_per_color = diamonds.groupby("color").median()

fig,ax = plt.subplots(1,3,figsize=(15, 5))
sns.scatterplot(data = mean_per_color, x = "color", y = "price", ax = ax[0])
ax[0].set_title("mean")
sns.scatterplot(data = median_per_color, x = "color",y = "price", color = "r", ax = ax[1])
ax[1].set_title("median")
sns.scatterplot(data = max_per_color, x = "color", y = "price", color = "g", ax = ax[2])
ax[2].set_title("max")
plt.show()

max_per_clarity = diamonds.groupby("clarity").max()
mean_per_clarity = diamonds.groupby("clarity").mean()
median_per_clarity = diamonds.groupby("clarity").median()

fig,ax = plt.subplots(1,3,figsize=(15, 5))
sns.scatterplot(data = mean_per_clarity, x = "clarity", y = "price", ax = ax[0])
ax[0].set_title("mean")
sns.scatterplot(data = median_per_clarity, x = "clarity",y = "price", color = "r", ax = ax[1])
ax[1].set_title("median")
sns.scatterplot(data = max_per_clarity, x = "clarity", y = "price", color = "g", ax = ax[2])
ax[2].set_title("max")
plt.show()

display(diamonds[diamonds["x"].isnull()])
display(diamonds[diamonds["y"].isnull()])
display(diamonds[diamonds["z"].isnull()])

"""diamonds["length_to_width_ratio"] = diamonds["y"]/diamonds["x"]
diamonds.describe()

First we assume x = y ,beacuse i'd taken a look on the width to length ratios and see that it the megority of it has 1:1 ratio which refer to rounded shape, then we compute z (Nan values) by using the total depth percentage formula.
"""

diamonds["x"] = np.where((diamonds["x"].isna()) & (~diamonds["y"].isna()),diamonds["y"],diamonds["x"])
diamonds["y"] = np.where((~diamonds["x"].isna()) & (diamonds["y"].isna()),diamonds["x"],diamonds["y"])
diamonds["z"] = np.where((~diamonds["x"].isna()) & (~diamonds["y"].isna())& (diamonds["z"].isna()),diamonds["depth"]*(diamonds["x"]+diamonds["y"])/200,diamonds["z"])

diamonds.info()

display(diamonds[diamonds["z"].isnull()])

diamonds.dropna(axis=0,inplace=True)

"""## Adding new featuers"""

diamonds["v"] = 1/12*np.pi*diamonds["z"]*(diamonds["x"]+diamonds["y"])**2/4*0.85+1/12*np.pi*diamonds["z"]*0.15*((diamonds["x"]+diamonds["y"])**2/4+((diamonds["x"]+diamonds["y"])*diamonds["table"]/200)**2+(diamonds["x"]+diamonds["y"])**2*diamonds["table"]/400)

fig, ax=plt.subplots(1,3,figsize=(25,10))
sns.boxplot(data=diamonds,x="price",y="cut",ax=ax[0])
ax[0].set_title("cut")
sns.boxplot(data=diamonds,x="price",y="color",ax=ax[1])
ax[1].set_title("color")
sns.boxplot(data=diamonds,x="price",y="clarity",ax=ax[2])
ax[2].set_title("clarity")
plt.show()

sns.scatterplot(data = diamonds,x="carat", y="price", hue="clarity")

fig , ax =plt.subplots(2,4,figsize=(25,10))
i=j=0
for var in diamonds["clarity"].unique():
  df = diamonds[diamonds["clarity"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="v", y="price",ax= ax[j][i])
  i=i+1
  if i == 4:
    j = 1
    i = 0

fig , ax =plt.subplots(2,4,figsize=(25,10))
i=j=0
for var in diamonds["clarity"].unique():
  df = diamonds[diamonds["clarity"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="carat", y="price",ax= ax[j][i])
  i=i+1
  if i == 4:
    j = 1
    i = 0

fig , ax =plt.subplots(2,3,figsize=(25,10))
i=j=0
for var in diamonds["cut"].unique():
  df = diamonds[diamonds["cut"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="v", y="price",ax= ax[j][i])
  i=i+1
  if i == 3:
    j = j+1
    i = 0

fig , ax =plt.subplots(2,3,figsize=(25,10))
i=j=0
for var in diamonds["cut"].unique():
  df = diamonds[diamonds["cut"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="carat", y="price",ax= ax[j][i])
  i=i+1
  if i == 3:
    j = j+1
    i = 0

fig , ax =plt.subplots(2,4,figsize=(25,10))
i=j=0
for var in diamonds["color"].unique():
  df = diamonds[diamonds["color"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="v", y="price",ax= ax[j][i])
  i=i+1
  if i == 4:
    j = j+1
    i = 0

fig , ax =plt.subplots(2,4,figsize=(25,10))
i=j=0
for var in diamonds["color"].unique():
  df = diamonds[diamonds["color"]==var]
  ax[j][i].set_title(var)
  sns.scatterplot(data = df,x="carat", y="price",ax= ax[j][i])
  i=i+1
  if i == 4:
    j = j+1
    i = 0

"""From the above figures, we note that the relation between (price, volume) and (price, carat) seems to follow a second-degree equation, so we need to add two new features carat^2 and volume^2."""

diamonds["v2"] = diamonds["v"]**2
diamonds["carat2"] = diamonds["carat"]**2

diamonds.describe()

"""## Features selection

This section is important to have a good knowledge about features contributions in the training process.

**Apply SelectKBest Algo**
"""

x_ = diamonds.iloc[:,diamonds.columns != "price"]
x_.drop("Unnamed: 0",axis=1,inplace=True)
y_ = diamonds[["price"]].copy()

catcut = {"Fair":0,"Good":1,"Very Good":2,"Premium":3,"Ideal":4}
x_["cut"] = x_["cut"].apply(lambda i: catcut[i])
catclarity = {'I1': 0, 'SI2' : 1, 'SI1' : 2, 'VS2' : 3, 'VS1' : 4, 'VVS2' : 5, 'VVS1' : 6 , 'IF' : 7}
x_["clarity"] = x_["clarity"].apply(lambda i: catclarity[i])
catcolor = {'J' : 0, 'I' : 1, 'H' : 2, 'G' : 3, 'F' : 4, 'E' : 5, 'D' : 6}
x_["color"] = x_["color"].apply(lambda i: catcolor[i])
x_

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

orderd_rank_features = SelectKBest(score_func=chi2,k= 12)
orderd_features = orderd_rank_features.fit(x_,y_)

dfscors = pd.DataFrame(orderd_features.scores_,columns=["score"])
dfcolumns = pd.DataFrame(x_.columns,columns=["features"])
features_rank = pd.concat([dfcolumns,dfscors], axis= 1)

features_rank.nlargest(12,"score")

"""**Information Gain**"""

from sklearn.feature_selection import mutual_info_regression

mutual_info = mutual_info_regression(x_,y_)
mutual_data = pd.Series(mutual_info,index=x_.columns)
print(mutual_data.sort_values(ascending=False))

"""**Features Importance based on Models**"""

from xgboost import XGBRegressor
from sklearn.ensemble import ExtraTreesRegressor
import matplotlib.pyplot as plt
model1 = ExtraTreesRegressor()
model2 = XGBRegressor(objective='reg:squarederror')
model1.fit(x_,y_)
model2.fit(x_,y_)

EXT_ranked_features = pd.Series(model1.feature_importances_,index=x_.columns)
XGB_ranked_features = pd.Series(model2.feature_importances_,index=x_.columns)

EXT_ranked_features.nlargest(12)

XGB_ranked_features.nlargest(12)

fig ,ax = plt.subplots(1,2,figsize=(14,7))
EXT_ranked_features.nlargest(12).plot(kind = "barh", ax = ax[0])
XGB_ranked_features.nlargest(12).plot(kind = "barh", ax = ax[1])
plt.show()

"""**Linear correlation**

using pearson corroletion
"""

catcut = {"Fair":0,"Good":1,"Very Good":2,"Premium":3,"Ideal":4}
diamonds["cut"] = diamonds["cut"].apply(lambda i: catcut[i])
catclarity = {'I1': 0, 'SI2' : 1, 'SI1' : 2, 'VS2' : 3, 'VS1' : 4, 'VVS2' : 5, 'VVS1' : 6 , 'IF' : 7}
diamonds["clarity"] = diamonds["clarity"].apply(lambda i: catclarity[i])
catcolor = {'J' : 0, 'I' : 1, 'H' : 2, 'G' : 3, 'F' : 4, 'E' : 5, 'D' : 6}
diamonds["color"] = diamonds["color"].apply(lambda i: catcolor[i])
diamonds

corr = diamonds.corr(method='pearson')#'pearson', 'kendall', 'spearman'
top_features = corr.index
plt.figure(figsize=(12,12))
sns.heatmap(diamonds[top_features].corr(method='pearson'),annot=True)
plt.show()

"""**linear Models with p-values and null hypothesies**"""

X_ = diamonds.iloc[:,diamonds.columns != "Unnamed: 0"]
std = StandardScaler()
X_ = pd.DataFrame(std.fit_transform(X_),columns = X_ .columns)

X_

y = X_["price"]
col_to_exclude = ["x","y","z","price"]
X_.drop(col_to_exclude,axis=1, inplace= True)
X = X_

import statsmodels.api as sma

LinearRegressionmo = LinearRegression()

X2  = sma.add_constant(X)
_0  = sma.OLS(y, X2).fit()
print(_0.summary())

"""We note that the p-values equal to zero for all features, which mean that they are dependent with target vector. 

But as we noticed that the used data is a non-linear data which make this test results is useless.

## Test/Train split
"""

diamonds.drop(["Unnamed: 0"],axis= 1 , inplace= True)

diamonds["v2_cut"] = pd.cut(diamonds["v2"],bins=[0.,2000,4000,6000,8000,10000,np.inf],labels=[1,2,3,4,5,6])
diamonds["v2_cut"].hist()

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1 , test_size=0.3, random_state= 107)
for train_index, test_val_index in split.split(diamonds,diamonds["v2_cut"]):
  strat_train_set = diamonds.iloc[train_index]
  strat_test_val_set = diamonds.iloc[test_val_index]

for set_ in (strat_train_set,strat_test_val_set):
  set_.drop("v2_cut",axis=1,inplace=True)

strat_test_val_set.info()

fig ,ax = plt.subplots(4,4,figsize=(20,20))
i = j = 0
for col in list(strat_train_set.columns):
  sns.distplot(strat_train_set[col],ax=ax[j][i]);
  i = i + 1
  if i == 4:
    j = j + 1
    i = 0
plt.show()

fig ,ax = plt.subplots(4,4,figsize=(20,20))
i = j = 0
for col in list(strat_test_val_set.columns):
  sns.distplot(strat_test_val_set[col],ax=ax[j][i]);
  i = i + 1
  if i == 4:
    j = j + 1
    i = 0
plt.show()

"""## Outliers Checking

**non parametric correlation**
"""

corr = strat_train_set.corr(method='spearman')#'pearson', 'kendall', 'spearman'
plt.figure(figsize=(10,10))
sns.heatmap(corr,annot=True)
plt.show()

corr = strat_train_set.corr(method='spearman')-strat_train_set.corr(method='kendall')#'pearson', 'kendall', 'spearman'
plt.figure(figsize=(13,13))
sns.heatmap(corr,annot=True)
plt.show()

"""We use non parametric correlation:




*   if (spearman_rho - kendall_tau > 0) there is no outliers between these two features.
*   if (spearman_rho - kendall_tau < 0) there is outliers between these two features.

**Distribution of the columns**
"""

fig ,ax = plt.subplots(4,4,figsize=(20,20))
i = j = 0
for col in list(strat_train_set.columns):
  sns.distplot(strat_train_set[col],ax=ax[j][i]);
  i = i + 1
  if i == 4:
    j = j + 1
    i = 0
plt.show()

"""**IQR removing outliers**"""

def outliers(df,column,a=25,b=75):
  x=df[column]
  q1, q3 = x.quantile(a/100),x.quantile(b/100)
  iqr = q3 - q1
  lower_bound = np.max([q1 - (1.5 * iqr),0])
  upper_bound = q3 + (1.5 * iqr)
  print("<<<<<<<<",col,">>>>>>>>")
  print(lower_bound)
  print(upper_bound)
  mask = x.between( lower_bound, upper_bound, inclusive="both")
  mask = mask[mask==False]
  return mask

df = strat_train_set.copy()
for col in (df.columns):
  out = outliers(df,col,a=10,b=90)
  df = df.drop(out.index,axis=0)
  print("min = ",df [col].min())
  print("max = ",df [col].max())

strat_train_set = df

strat_train_set.info()

corr = strat_train_set.corr(method='spearman')-strat_train_set.corr(method='kendall')#'pearson', 'kendall', 'spearman'
plt.figure(figsize=(13,13))
sns.heatmap(corr,annot=True)
plt.show()

"""**Distribution of the Filtered columns**"""

fig ,ax = plt.subplots(4,4,figsize=(20,20))
i = j = 0
for col in list(strat_train_set.columns):
  sns.distplot(strat_train_set[col],ax=ax[j][i]);
  i = i + 1
  if i == 4:
    j = j + 1
    i = 0
plt.show()

X_ = strat_train_set.copy()
std = StandardScaler()
X_ = pd.DataFrame(std.fit_transform(X_),columns = X_ .columns)
fig ,ax = plt.subplots(4,4,figsize=(20,20))
i = j = 0
for col in list(X_.columns):
  sns.distplot(X_[col],ax=ax[j][i]);
  i = i + 1
  if i == 4:
    j = j + 1
    i = 0
plt.show()

"""**skewness & kurtosis**"""

from scipy.stats import skew ,kurtosis

for col in X_:
  print(col)
  print("skew : ",skew(X_[col]))
  print("kurtosis : ",kurtosis(X_[col]))
  plt.figure()
  sns.distplot(X_[col])
  plt.show()

num_columns= list(strat_train_set.columns)
strat_train_set.columns

class columnDropperTransformer(BaseEstimator,TransformerMixin):
    def __init__(self,columns):
        self.columns=columns

    def transform(self,X,y=None):
        return X.drop(self.columns,axis=1)

    def fit(self, X, y=None):
        return self

col_to_exclude = ["x","y","z","price"]

def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())

y_train = strat_train_set["price"].copy()

"""**Important question could anyone ask:**

Why I'm removing outliers from the test set??

The answer is obvious: the test set must contain correct values and the outliers I'll remove are definitely nonrealistic diamonds.
"""

def outliers(df,column,a=25,b=75):
  x=df[column]
  q1, q3 = x.quantile(a/100),x.quantile(b/100)
  iqr = q3 - q1
  lower_bound = np.max([q1 - (1.5 * iqr),0])
  upper_bound = q3 + (1.5 * iqr)
  print("<<<<<<<<",col,">>>>>>>>")
  print(lower_bound)
  print(upper_bound)
  mask = x.between( lower_bound, upper_bound, inclusive="both")
  mask = mask[mask==False]
  return mask

df = strat_test_val_set.copy()
for col in (df.columns):
  out = outliers(df,col,a=5,b=95)
  df = df.drop(out.index,axis=0)
  print("min = ",df [col].min())
  print("max = ",df [col].max())

strat_test_val_set2 = df

"""## Finding the Best combination

Running It, takes a long time.

strategies = ['mean' , 'median']
encoders_1 = {"OrdinalEncoder":OrdinalEncoder() ,"OneHotEncoder":OneHotEncoder() } #,"LabelEncoder":LabelEncoder()
encoders_2 = {"OrdinalEncoder":OrdinalEncoder() ,"OneHotEncoder":OneHotEncoder() } #,"LabelEncoder":LabelEncoder()
encoders_3 = {"OrdinalEncoder":OrdinalEncoder() ,"OneHotEncoder":OneHotEncoder() } #,"LabelEncoder":LabelEncoder()
scalers = {"MinMaxScaler":MinMaxScaler(),"StandardScaler":StandardScaler(),"MaxAbsScaler":MaxAbsScaler(),"RobustScaler":RobustScaler(),"QuantileTransformer (uniform)":QuantileTransformer(output_distribution='uniform'),"QuantileTransformer (normal)":QuantileTransformer(output_distribution='normal'),"PowerTransformer (yeo-johnson)":PowerTransformer(method="yeo-johnson"),"PowerTransformer (box-cox)":PowerTransformer(method="box-cox")}
models = {"LinearRegression":LinearRegressionmo,"Lasso":Lassomo,"Ridge":Ridgemo,"DecisionTreeRegressor":DecisionTreeRegressormo,"RandomForestRegressor":RandomForestRegressormo,"XGBRegressor":XGBRegressormo}
result_dec = {"strategy":[],"model":[],"scaler":[],"encoder_1":[],"encoder_2":[],"encoder_3":[],"RMSE":[],"STD":[]}
i=0
for stra in strategies:
  for e1_key, e1_mod in zip(encoders_1.keys(),encoders_1.values()):
    for e2_key, e2_mod in zip(encoders_2.keys(),encoders_2.values()):
      for e3_key, e3_mod in zip(encoders_3.keys(),encoders_3.values()):
        for s_key, s_mod in zip(scalers.keys(),scalers.values()):
          for m_key, m_mod in zip(models.keys(),models.values()):
            numerical_Pipeline = Pipeline([
                ("columnDropper",columnDropperTransformer(col_to_exclude)),
                ("imputer",SimpleImputer(strategy=stra)),
                ("std_scaler",s_mod)
                ])
            full_pipline = ColumnTransformer([
                ("cat1",e1_mod,["cut"]),
                ("cat2",e2_mod,['clarity']),
                ("cat3",e3_mod,["color"]),
                ("num",numerical_Pipeline,num_columns)
                ])
            diamonds_prepared = full_pipline.fit_transform(diamonds)
            m_mod.fit(diamonds_prepared,target_data)
            scores = cross_val_score(m_mod,diamonds_prepared,target_data,scoring="neg_mean_squared_error", cv=4)
            rmse_scores = np.sqrt(-scores)
            print("\n<<<<<<<<<<<<<<<<<<<<<<<<<<<",i,">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n")
            print(np.shape(diamonds_prepared))
            print(stra,m_key,s_key,e1_key,e2_key,e3_key,"\n")
            display_scores(rmse_scores)
            result_dec["strategy"].append(stra)
            result_dec["model"].append(m_key)
            result_dec["scaler"].append(s_key)
            result_dec["encoder_1"].append(e1_key)
            result_dec["encoder_2"].append(e2_key)
            result_dec["encoder_3"].append(e3_key)
            result_dec["RMSE"].append(rmse_scores.mean())
            result_dec["STD"].append(rmse_scores.std())
            i=i+1

## Training & Testing
"""

y_test = strat_test_val_set2["price"].copy()

numerical_Pipeline = Pipeline([
    ("columnDropper",columnDropperTransformer(col_to_exclude)),
    ("imputer",SimpleImputer(strategy="median")),
    ("standrization",StandardScaler())
    ])
X_train = numerical_Pipeline.fit_transform(strat_train_set)
X_test = numerical_Pipeline.fit_transform(strat_test_val_set2)

print(X_train)
print(X_test)

print(X_train.shape)
print(X_test.shape)



strat_test_val_set2["v2"].nlargest(100)

sns.distplot(strat_test_val_set["v2"])

sns.distplot(strat_train_set["v2"])

lr = LinearRegression()
Lassomo = Lasso(alpha=0.001,max_iter=10000)
Ridgemo = Ridge(alpha=0.001,max_iter=10000)
Tlr = TransformedTargetRegressor(regressor=lr,func=np.log,inverse_func=np.exp)
TLassomo = TransformedTargetRegressor(regressor=Lassomo,func=np.log,inverse_func=np.exp)
TRidgemo = TransformedTargetRegressor(regressor=Ridgemo,func=np.log,inverse_func=np.exp)
dt = DecisionTreeRegressor()
rn = RandomForestRegressor()
extr= ExtraTreesRegressor()
GBr = GradientBoostingRegressor()
XGB = XGBRegressor( objective= 'reg:squarederror')
AdaBR1 = AdaBoostRegressor(base_estimator = XGB,learning_rate=0.01,loss='square')
AdaBR2 = AdaBoostRegressor(base_estimator = dt,learning_rate=0.01,loss='square')


models = [lr,Tlr,Lassomo,TLassomo,Ridgemo,TRidgemo,dt,rn,extr,GBr,XGB,AdaBR1,AdaBR2]
r2 = {}
rmse = {}
l = 1
for i in models:
    i.fit(X_train,y_train)
    ypred = i.predict(X_test)
    err = np.sqrt(mean_squared_error(y_test, ypred))
    rmse.update({str(i):err})
    print(i,":",r2_score(ypred,y_test)*100)
    r2.update({str(i):r2_score(ypred,y_test)*100})

rmse

"""Averaging Models predictions to improve the performance."""

models = [rn,AdaBR2]
r2 = {}
rmse = {}
l = 1
for i in models:
    i.fit(X_train,y_train)
    ypred = i.predict(X_test)
    if l == 1:
      sumpred = ypred
    else:
      sumpred = sumpred + ypred
    sum_pred = sumpred/l
    err = np.sqrt(mean_squared_error(y_test, sum_pred))
    rmse.update({str(l):err})
    print(l,":",r2_score(sum_pred,y_test)*100)
    r2.update({str(l):i.score(X_test,y_test)*100})
    l = l + 1

rmse